# Flourish Transcript Viewer

Interactive web-based viewer for exploring Flourish evaluation results.

## Features

- **Transcript Browser**: View model responses, prompts, and judge reasoning with pagination
- **Filtering**: Filter by virtue, model, score range, and search text
- **Model Comparison**: Compare how different models performed on the same scenario
- **Analytics Dashboard**: Visualize score distributions, model performance, and virtue-level metrics
- **Export**: Download filtered results as CSV

## Installation

```bash
# From the flourish project root
pip install -r viewer/requirements.txt
```

## Quick Start

```bash
# Run the viewer
streamlit run viewer/app.py

# The app will open in your browser at http://localhost:8501
```

## Usage

### Loading Data

**Option 1: File Upload**
1. Click "Browse files" in the sidebar
2. Select a Flourish results CSV file

**Option 2: File Path**
1. Enter the path to your CSV file (e.g., `results/detailed_20260126.csv`)
2. Click "Load CSV"

### Filtering

Use the sidebar filters to narrow down transcripts:
- **Virtues**: Select which virtues to include
- **Models**: Select which models to include
- **Score Range**: Filter by score (0-2)
- **Search**: Search text in prompts, responses, or justifications
- **Exclude errors**: Hide transcripts with evaluation errors

### View Modes

**Transcripts View**
- Browse individual transcripts with pagination
- Expand prompts and judge summaries
- View highlights and metadata

**Comparison View**
- Select a scenario to compare
- See all models' responses side-by-side
- View score differences across models

**Analytics View**
- Overall statistics and score distribution
- Model Ã— Virtue performance heatmap
- Per-virtue and per-model breakdowns
- Score distribution charts

### Exporting

Click "Export Filtered Data" in the sidebar to download currently filtered results as CSV.

## Data Format

The viewer expects CSV files with the following columns:

| Column | Type | Description |
|--------|------|-------------|
| `scenario_id` | str | Unique scenario identifier |
| `prompt` | str | The prompt given to the model |
| `response` | str | Model's response |
| `score` | int | Judge score (0-2) |
| `virtue` | str | Virtue being evaluated |
| `model` | str | Model being evaluated |
| `judge` | str | Judge model used for scoring |
| `justification` | str | Judge's reasoning (optional) |
| `summary` | str | Judge summary (optional) |
| `highlights` | str/JSON | Key highlights (optional) |
| `timestamp` | str | Evaluation timestamp (optional) |
| `error` | bool | Error flag (optional) |

This format is automatically generated by `flourish.evaluator.run_eval_suite()`.

## Examples

### Generate Results to View

```bash
# Run evaluations
flourish --model claude-sonnet-4 --eval evals/empathy_in_action.yaml --output results/

# Or run multiple models
python -c "
from flourish.evaluator import run_full_evaluation

results = run_full_evaluation(
    models=['claude-sonnet-4', 'gpt-4'],
    eval_files=['evals/empathy_in_action.yaml'],
    output_dir='results/'
)
"

# View results
streamlit run viewer/app.py
```

### Programmatic Usage

```python
from viewer.data_loader import FlourishDataLoader

# Load data
loader = FlourishDataLoader("results/detailed_20260126.csv")

# Get statistics
stats = loader.get_summary_stats()
print(f"Average score: {stats['avg_score']:.2f}")

# Filter data
filtered = loader.filter_data(
    virtues=["Empathy in Action"],
    models=["claude-sonnet-4"],
    score_range=(1, 2)
)

# Get model comparison
comparison = loader.get_scenario_comparison("eia_001")
```

## Architecture

Inspired by [Bloom's transcript viewer](https://github.com/safety-research/bloom) but adapted for Flourish's:
- Single-turn scenarios (vs multi-turn conversations)
- 0-2 scoring scale (vs 1-10)
- Curated scenarios (vs dynamic generation)
- CSV format (vs JSON transcripts)

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed design decisions.

## Troubleshooting

**"No module named 'viewer'"**
- Make sure to run from the flourish project root: `streamlit run viewer/app.py`

**"Missing required columns"**
- Ensure your CSV has `scenario_id`, `prompt`, `response`, `score`, `virtue`, and `model` columns

**Charts not displaying**
- Check that plotly is installed: `pip install plotly`

**Upload fails**
- Try using the file path input instead
- Verify the CSV file is valid and not corrupted

## License

Same as Flourish project. Components adapted from Bloom (MIT License).
