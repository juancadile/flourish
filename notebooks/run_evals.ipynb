{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Flourish: Virtue Evaluation Suite\n\nThis notebook runs behavioral evaluations across multiple LLM models to measure virtuous traits:\n\n1. **Empathy in Action** - Does the model show genuine empathy when encountering distress?\n2. **Intellectual Humility** - Does the model admit uncertainty and update beliefs?\n3. **Honesty Under Pressure** - Does the model tell the truth even when uncomfortable?\n4. **Caretaking** - Does the model proactively look out for the user's wellbeing?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you have API keys set as environment variables:\n",
    "- `ANTHROPIC_API_KEY`\n",
    "- `OPENAI_API_KEY`\n",
    "- `GOOGLE_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from flourish import VirtueEvaluator\n",
    "from flourish.evaluator import run_full_evaluation, aggregate_results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Models to evaluate\nMODELS = [\n    'claude-sonnet-4',\n    'gpt-4o',\n    'gemini-2.0-flash',\n]\n\n# Evaluation files\nEVAL_FILES = [\n    '../evals/empathy_in_action.yaml',\n    '../evals/intellectual_humility.yaml',\n    '../evals/honesty_under_pressure.yaml',\n    '../evals/caretaking.yaml',\n]\n\n# Judge model for scoring\nJUDGE_MODEL = 'claude-sonnet-4'\n\n# Output directory\nOUTPUT_DIR = Path('../results')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run Evaluations\n\nThis will run all scenarios (60 per model = 180 total evaluations across 3 models)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation suite\n",
    "results = run_full_evaluation(\n",
    "    models=MODELS,\n",
    "    eval_files=EVAL_FILES,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal evaluations: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary = aggregate_results(results)\n",
    "print(\"\\nAverage Scores by Model and Virtue (0-2 scale):\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "summary.to_csv(OUTPUT_DIR / 'summary.csv')\n",
    "results.to_csv(OUTPUT_DIR / 'detailed_results.csv', index=False)\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data for heatmap (exclude 'average' column)\n",
    "heatmap_data = summary.drop(columns=['average'], errors='ignore')\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0,\n",
    "    vmax=2,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Score (0-2)'}\n",
    ")\n",
    "\n",
    "ax.set_title('Virtue Scores by Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Virtue', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing models\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Reshape data for grouped bar chart\n",
    "plot_data = results.groupby(['model', 'virtue'])['score'].mean().unstack()\n",
    "\n",
    "plot_data.plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    width=0.8,\n",
    "    edgecolor='white',\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "ax.set_title('Model Performance by Virtue', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Average Score (0-2)', fontsize=12)\n",
    "ax.set_ylim(0, 2.2)\n",
    "ax.legend(title='Virtue', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'bar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Score distribution per virtue\nvirtues = results['virtue'].unique()\nn_virtues = len(virtues)\n\nfig, axes = plt.subplots(1, n_virtues, figsize=(5 * n_virtues, 5))\n\nfor ax, virtue in zip(axes, virtues):\n    virtue_data = results[results['virtue'] == virtue]\n    \n    # Count scores by model\n    score_counts = virtue_data.groupby(['model', 'score']).size().unstack(fill_value=0)\n    \n    score_counts.plot(\n        kind='bar',\n        stacked=True,\n        ax=ax,\n        color=['#e74c3c', '#f39c12', '#27ae60'],\n        edgecolor='white',\n    )\n    \n    ax.set_title(virtue, fontsize=12, fontweight='bold')\n    ax.set_xlabel('Model', fontsize=10)\n    ax.set_ylabel('Count', fontsize=10)\n    ax.legend(title='Score', labels=['0', '1', '2'])\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n\nplt.suptitle('Score Distribution by Virtue', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'distribution.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show lowest scoring scenarios across all models\n",
    "print(\"\\nLowest Scoring Scenarios (score=0):\")\n",
    "low_scores = results[results['score'] == 0][['model', 'virtue', 'scenario_id', 'reasoning']]\n",
    "if len(low_scores) > 0:\n",
    "    display(low_scores)\n",
    "else:\n",
    "    print(\"No scenarios scored 0!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show highest scoring scenarios\n",
    "print(\"\\nHighest Scoring Scenarios (score=2):\")\n",
    "high_scores = results[results['score'] == 2][['model', 'virtue', 'scenario_id', 'reasoning']]\n",
    "print(f\"Total: {len(high_scores)} scenarios scored 2/2\")\n",
    "display(high_scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison statistics\n",
    "print(\"\\nStatistics by Model:\")\n",
    "stats = results.groupby('model')['score'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "stats.columns = ['Mean', 'Std Dev', 'Min', 'Max', 'Count']\n",
    "display(stats.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Responses\n",
    "\n",
    "Let's examine some example responses to understand the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample responses with different scores\n",
    "def show_sample(df, model, virtue, score):\n",
    "    sample = df[(df['model'] == model) & (df['virtue'] == virtue) & (df['score'] == score)]\n",
    "    if len(sample) > 0:\n",
    "        row = sample.iloc[0]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Model: {row['model']} | Virtue: {row['virtue']} | Score: {row['score']}\")\n",
    "        print(f\"Scenario: {row['scenario_id']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nPrompt:\\n{row['prompt'][:500]}...\")\n",
    "        print(f\"\\nResponse:\\n{row['response'][:500]}...\")\n",
    "        print(f\"\\nJudge Reasoning: {row['reasoning']}\")\n",
    "\n",
    "# Show examples if we have results\n",
    "if len(results) > 0:\n",
    "    model = results['model'].iloc[0]\n",
    "    virtue = results['virtue'].iloc[0]\n",
    "    for score in [0, 1, 2]:\n",
    "        show_sample(results, model, virtue, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary for README\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(\"\\nTable for README:\")\nprint(\"\\n| Model | Empathy | Humility | Honesty | Caretaking | Average |\")\nprint(\"|-------|---------|----------|---------|------------|---------|\")\nfor model in summary.index:\n    row = summary.loc[model]\n    virtues = [v for v in row.index if v != 'average']\n    scores = [f\"{row[v]:.2f}\" for v in virtues]\n    avg = f\"{row['average']:.2f}\" if 'average' in row.index else 'N/A'\n    print(f\"| {model} | {' | '.join(scores)} | {avg} |\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}